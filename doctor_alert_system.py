# -*- coding: utf-8 -*-
"""Doctor Alert System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ovtnnECfJp50wNkO495rY_MhstMiawHO

# **Install**
"""

!pip install datasets
!pip install jiwer
!pip install gTTS
!pip install --upgrade openai-whisper
!pip install transformers accelerate torch sentencepiece fuzzywuzzy

"""# **Drive Mount**"""

from google.colab import drive
drive.mount('/content/drive')

"""# **MedDialog Dataset Load**"""

from datasets import load_dataset

# Load 10k English MedDialog dataset
dataset = load_dataset("BinKhoaLe1812/MedDialog-EN-10k")

print(dataset)
print(dataset["train"][0])

"""# **converting to audio**"""

from gtts import gTTS
import os, time, re

output_dir = "/content/drive/MyDrive/DASYSTEM2/synthetic_audio_gtts"
os.makedirs(output_dir, exist_ok=True)

for i, row in enumerate(dataset["train"]):
    try:
        # Clean and get text
        patient_text = row["input"].strip()
        doctor_text = row["answer_icliniq"].strip()

        # Skip if no text
        if not patient_text and not doctor_text:
            print(f"âš ï¸ Index {i} has no text. Skipping.")
            continue

        # Optional: add slow speech trick with punctuation
        def slowify(text):
            text = text.replace(",", ",,").replace(".", "..")
            return text

        if patient_text:
            tts = gTTS(slowify(patient_text), lang="en", slow=True)
            tts.save(os.path.join(output_dir, f"patient_{i}.mp3"))
            print(f"ðŸŽ§ Saved patient_{i}.mp3")


        time.sleep(1)  # small delay

    except Exception as e:
        print(f"âš ï¸ Error at index {i}: {e}")
        time.sleep(5)

"""**Audio Preprocessing**"""

from pydub import AudioSegment
import os

# Directory with synthetic audio
input_dir = "/content/drive/MyDrive/DASYSTEM2/synthetic_audio_gtts"
processed_dir = "/content/drive/MyDrive/DASYSTEM2/synthetic_audio_processed"
os.makedirs(processed_dir, exist_ok=True)

for file in os.listdir(input_dir):
    if file.endswith(".mp3"):
        audio = AudioSegment.from_file(os.path.join(input_dir, file))
        audio = audio.set_frame_rate(16000).set_channels(1)
        output_file = os.path.join(processed_dir, file.replace(".mp3", ".wav"))
        audio.export(output_file, format="wav")
        print(f"Processed {file} -> {output_file}")

"""# **Transcripted**"""

import os
import whisper
model = whisper.load_model("large")  # or "large"


input_dir = "/content/drive/MyDrive/DASYSTEM2/synthetic_audio_processed"
transcripts_dir = "/content/drive/MyDrive/DASYSTEM2/transcript large"
os.makedirs(transcripts_dir, exist_ok=True)

for file in os.listdir(input_dir):
    if file.endswith(".wav"):
        result = model.transcribe(os.path.join(input_dir, file))
        text = result['text']
        # Save transcript
        with open(os.path.join(transcripts_dir, file.replace(".wav", ".txt")), "w") as f:
            f.write(text)
        print(f"Transcribed {file} -> {text[:50]}...")

"""# **Text Profiling / Risk Analysis using fine tuned biobert**"""

!pip install transformers torch scikit-learn pandas tqdm

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from torch import nn, optim
import torch.nn.functional as F
from sklearn.preprocessing import LabelEncoder
from tqdm import tqdm
from google.colab import drive

# ---------- CONFIG ----------
MODEL_NAME = "dmis-lab/biobert-base-cased-v1.2"
USE_GPU = torch.cuda.is_available()
DEVICE = "cuda" if USE_GPU else "cpu"
BATCH_SIZE = 8
EPOCHS = 3
MAX_LEN = 512
LEARNING_RATE = 2e-5

# ---------- Load dataset from Drive ----------
data_path = "/content/drive/MyDrive/DASYSTEM/risk.csv"
df = pd.read_csv(data_path)

# Encode labels to integers: Low=0, Medium=1, High=2
le = LabelEncoder()
df['label_id'] = le.fit_transform(df['label'])

# ---------- Dataset class ----------
class RiskDataset(Dataset):
    def __init__(self, texts, labels, tokenizer):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=MAX_LEN,
            return_tensors='pt'
        )
        item = {key: val.squeeze(0) for key, val in encoding.items()}
        item['labels'] = torch.tensor(label, dtype=torch.long)
        return item

# ---------- Load tokenizer and base model ----------
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
base_model = AutoModel.from_pretrained(MODEL_NAME)

# ---------- Classification head ----------
class BioBERTClassifier(nn.Module):
    def __init__(self, base_model, num_labels=3):
        super(BioBERTClassifier, self).__init__()
        self.base = base_model
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Linear(base_model.config.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

model = BioBERTClassifier(base_model).to(DEVICE)

# ---------- Dataloader ----------
dataset = RiskDataset(df['text'].tolist(), df['label_id'].tolist(), tokenizer)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# ---------- Optimizer ----------
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)

# ---------- Training loop ----------
for epoch in range(EPOCHS):
    model.train()
    loop = tqdm(dataloader, leave=True)
    total_loss = 0
    for batch in loop:
        input_ids = batch['input_ids'].to(DEVICE)
        attention_mask = batch['attention_mask'].to(DEVICE)
        labels = batch['labels'].to(DEVICE)

        optimizer.zero_grad()
        logits = model(input_ids, attention_mask)
        loss = F.cross_entropy(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        loop.set_description(f"Epoch {epoch+1}")
        loop.set_postfix(loss=loss.item())

    print(f"Epoch {epoch+1} completed. Avg loss: {total_loss/len(dataloader):.4f}")

# ---------- Save model ----------
save_dir = "/content/drive/MyDrive/DASYSTEM/biobert_risk_model"
model_file = save_dir + "/biobert_risk_classifier.pt"
tokenizer_dir = save_dir + "/tokenizer"
import os
os.makedirs(save_dir, exist_ok=True)

torch.save(model.state_dict(), model_file)
tokenizer.save_pretrained(tokenizer_dir)
print("Model fine-tuned and saved to Drive!")

import os
import json
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
from fuzzywuzzy import fuzz
from sklearn.preprocessing import LabelEncoder
import pandas as pd

# ---------- CONFIG ----------
transcripts_dir = "/content/drive/MyDrive/DASYSTEM2/transcript large"
output_dir = "/content/drive/MyDrive/DASYSTEM2/profile_results_biobert_patient_large"
os.makedirs(output_dir, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MAX_LEN = 512
patient_prefix = "patient_"

# ---------- Keyword rules ----------
keywords_high_risk = ["chest pain", "shortness of breath", "severe bleeding"]
keywords_medium_risk = ["dizziness", "headache", "fever"]
FUZZY_THRESHOLD = 85

# ---------- Load tokenizer ----------
tokenizer = AutoTokenizer.from_pretrained("/content/drive/MyDrive/DASYSTEM2/biobert_risk_model/tokenizer")

# ---------- Define model class ----------
class BioBERTClassifier(torch.nn.Module):
    def __init__(self, base_model, num_labels=3):
        super(BioBERTClassifier, self).__init__()
        self.base = base_model
        self.dropout = torch.nn.Dropout(0.3)
        self.classifier = torch.nn.Linear(base_model.config.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

# ---------- Load base model ----------
base_model = AutoModel.from_pretrained("dmis-lab/biobert-base-cased-v1.2")
model = BioBERTClassifier(base_model).to(DEVICE)

# ---------- Load fine-tuned weights ----------
model.load_state_dict(torch.load("/content/drive/MyDrive/DASYSTEM2/biobert_risk_model/biobert_risk_classifier.pt", map_location=DEVICE))
model.eval()

# ---------- Load LabelEncoder ----------
df_labels = pd.read_csv("/content/drive/MyDrive/DASYSTEM2/risk.csv")
le = LabelEncoder()
le.fit(df_labels['label'])

# ---------- Risk prediction ----------
def predict_risk(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=MAX_LEN).to(DEVICE)
    with torch.no_grad():
        logits = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])
        pred_label = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()
    return le.inverse_transform([pred_label])[0]

# ---------- Hybrid risk detection ----------
def detect_risk(text):
    text_lower = text.lower()
    # High-risk keywords
    for kw in keywords_high_risk:
        if fuzz.partial_ratio(kw.lower(), text_lower) >= FUZZY_THRESHOLD:
            return "High"
    # Medium-risk keywords
    for kw in keywords_medium_risk:
        if fuzz.partial_ratio(kw.lower(), text_lower) >= FUZZY_THRESHOLD:
            return "Medium"
    # Otherwise, use BioBERT prediction
    return predict_risk(text)

# ---------- Process all patient transcripts ----------
for fname in os.listdir(transcripts_dir):
    if not fname.endswith(".txt") or not fname.startswith(patient_prefix):
        continue

    file_path = os.path.join(transcripts_dir, fname)
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read().strip()

    risk = detect_risk(text)

    # Save result
    out_path = os.path.join(output_dir, fname.replace(".txt", ".json"))
    with open(out_path, "w", encoding="utf-8") as out_f:
        json.dump({
            "file": fname,
            "risk_level": risk,
            "text": text
        }, out_f, ensure_ascii=False, indent=2)

    print(f"{fname} -> {risk}")

"""# **Alert System via Mail**"""

import os
import json
import time
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# --- Gmail Configuration ---
GMAIL_USER = "xyz@gmail.com"
GMAIL_APP_PASSWORD = "abcdisn"  # App Password required

# --- Function to Send Alerts via Gmail SSL ---
def send_alert_smtplib(to_email, subject, message, retries=1):
    attempt = 0
    while attempt <= retries:
        try:
            # Create email message
            msg = MIMEMultipart()
            msg['From'] = GMAIL_USER
            msg['To'] = to_email
            msg['Subject'] = subject
            msg.attach(MIMEText(message, 'plain'))

            # Connect using SSL (port 465)
            server = smtplib.SMTP_SSL('smtp.gmail.com', 465)
            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)
            server.send_message(msg)
            server.quit()

            print(f"ðŸ“§ Alert sent to {to_email} (Attempt {attempt+1})")
            return True
        except Exception as e:
            print(f"âš ï¸ Attempt {attempt+1} failed to send email to {to_email}: {e}")
            attempt += 1
            time.sleep(5)  # wait before retrying

    return False


# --- JSON Alert Loop ---
doctor_email = "xyz@gmail.com"
profile_dir = "/content/drive/MyDrive/DASYSTEM2/profile_results_biobert_patient_large"

for file in os.listdir(profile_dir):
    if file.endswith(".json"):
        file_path = os.path.join(profile_dir, file)
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        risk = data.get("risk_level", "Low")
        transcript = data.get("text", "")

        if risk in ["Medium", "High"]:
            subject = f"âš ï¸ {risk} Risk Alert from Patient Audio"
            message = f"Detected {risk} risk in patient audio.\n\nTranscript:\n{transcript}"

            success = send_alert_smtplib(doctor_email, subject, message)
            if success:
                print(f"âœ… ALERT SENT for {file}: {risk}")
            else:
                print(f"âŒ Failed to send alert for {file}: {risk}")

            time.sleep(5)  # small delay to avoid SMTP rate limits
        else:
            print(f"{file} â†’ Low (No alert sent)")

"""# **Evaluation**"""

from datasets import load_dataset
import os

# ---------- Config ----------
output_dir = "/content/drive/MyDrive/DASYSTEM2/patient_transcripts_168"
os.makedirs(output_dir, exist_ok=True)

patient_prefix = "patient_"
num_patients = 168  # limit to first 168

# ---------- Load dataset ----------
dataset = load_dataset("BinKhoaLe1812/MedDialog-EN-10k")
train_data = dataset["train"]

# ---------- Save first 168 patient utterances ----------
for idx in range(num_patients):
    sample = train_data[idx]

    if isinstance(sample, dict):
        patient_text = sample["input"].strip()
    else:
        patient_text = sample["input"].strip()

    filename = f"{patient_prefix}{idx}.txt"
    path = os.path.join(output_dir, filename)

    with open(path, "w", encoding="utf-8") as f:
        f.write(patient_text)

print(f"Saved {num_patients} patient transcripts to {output_dir}")

import os
import time
from jiwer import wer
from tqdm import tqdm

# --- Paths ---
transcripts_dir = "/content/drive/MyDrive/DASYSTEM2/transcript large"
profile_dir = "/content/drive/MyDrive/DASYSTEM2/profile_results_biobert_patient_large"
dataset_dir = "/content/drive/MyDrive/DASYSTEM2/synthetic_audio_gtts"

# --- Load Dataset ---
from datasets import load_dataset
dataset = load_dataset("BinKhoaLe1812/MedDialog-EN-10k")

# --- Risk Keywords ---
keywords_high_risk = ["chest pain", "shortness of breath", "severe bleeding"]
keywords_medium_risk = ["dizziness", "headache", "fever"]

# --- Storage ---
wer_scores = []
alert_log = []

# --- Evaluation Loop ---
start_time = time.time()
for i in tqdm(range(300), desc="Evaluating patient transcripts"):
    patient_file = os.path.join(transcripts_dir, f"patient_{i}.txt")

    # Skip missing transcripts
    if not os.path.exists(patient_file):
        continue

    # Load transcript
    with open(patient_file, "r") as f:
        patient_transcript = f.read().strip()

    # Original reference text
    patient_ref = dataset["train"][i]["input"].strip()

    # WER
    patient_wer = wer(patient_ref, patient_transcript)
    wer_scores.append(patient_wer)

    # --- Risk Detection ---
    text_lower = patient_transcript.lower()
    predicted_risk = "Low"
    for kw in keywords_high_risk:
        if kw in text_lower:
            predicted_risk = "High"
            break
    if predicted_risk == "Low":
        for kw in keywords_medium_risk:
            if kw in text_lower:
                predicted_risk = "Medium"
                break


    profile_file = os.path.join(profile_dir, f"patient_{i}.txt")
    if os.path.exists(profile_file):
        with open(profile_file, "r") as f:
            lines = f.readlines()
        true_risk = lines[0].split(":")[1].strip()
    else:
        true_risk = "Low"

    # --- Alerts ---
    send_alert = predicted_risk in ["High", "Medium"]
    alert_log.append({
        "file": patient_file,
        "predicted_risk": predicted_risk,
        "true_risk": true_risk,
        "alert_sent": send_alert
    })

end_time = time.time()
latency = (end_time - start_time) / len(alert_log) if alert_log else 0

# --- Transcription Accuracy ---
avg_patient_wer = sum(wer_scores) / len(wer_scores)

# --- Alert Reliability ---
high_alerts = [a for a in alert_log if a["predicted_risk"]=="High"]
high_alerts_sent = [a for a in high_alerts if a["alert_sent"]]
high_alert_reliability = len(high_alerts_sent) / len(high_alerts) * 100 if high_alerts else 0

medium_alerts = [a for a in alert_log if a["predicted_risk"]=="Medium"]
medium_false_positives = [a for a in medium_alerts if not a["alert_sent"]]
medium_false_positive_rate = len(medium_false_positives) / len(medium_alerts) * 100 if medium_alerts else 0

# --- Scalability ---
scalability = len(alert_log) / (end_time - start_time) if alert_log else 0


print("=== Transcription Accuracy ===")
print(f"Average Patient WER: {avg_patient_wer:.2%}")

print("\n=== Alert Reliability ===")
print(f"High-risk alerts delivered: {high_alert_reliability:.2f}%")
print(f"Medium-risk false positive rate: {medium_false_positive_rate:.2f}%")

print("\n=== Latency ===")
print(f"Average processing time per file: {latency:.2f}s")

print("\n=== Scalability ===")
print(f"Files processed per second: {scalability:.2f}")

import os
import json
import pandas as pd
from sklearn.metrics import precision_recall_fscore_support


profile_dir = "/content/drive/MyDrive/DASYSTEM2/profile_results_biobert_patient_large"
ground_truth_csv = "/content/drive/MyDrive/DASYSTEM2/ground_truth_21.csv"

# ---------- Load ground truth ----------
df_gt = pd.read_csv(ground_truth_csv)  # columns: patient_id,risk_level
gt_dict = dict(zip(df_gt['patient_id'], df_gt['risk_level']))

# ---------- Load predicted risk from JSONs ----------
y_true = []
y_pred = []

for patient_id in df_gt['patient_id']:
    json_file = os.path.join(profile_dir, f"patient_{patient_id}.json")
    if os.path.exists(json_file):
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        predicted_risk = data.get("risk_level", "Low")
    else:
        predicted_risk = "Low"  # fallback if file missing
    y_true.append(gt_dict[patient_id])
    y_pred.append(predicted_risk)

# ---------- Compute Profiling Accuracy ----------
precision, recall, f1, _ = precision_recall_fscore_support(
    y_true, y_pred, labels=["High","Medium","Low"], average="weighted"
)

print("=== Profiling Accuracy (21 patients) ===")
print(f"Precision: {precision:.2%}")
print(f"Recall:    {recall:.2%}")
print(f"F1-score:  {f1:.2%}")

"""# **Database**

Connect to a new SQLite **database**
"""

import sqlite3
import os
import json
import time

# Connect (or create) database
conn = sqlite3.connect('/content/drive/MyDrive/DASYSTEM2/doctor_alert.db')
cursor = conn.cursor()

# Create tables
cursor.execute("""
CREATE TABLE IF NOT EXISTS transcripts (
    patient_id TEXT PRIMARY KEY,
    transcript TEXT,
    timestamp TEXT
)
""")

cursor.execute("""
CREATE TABLE IF NOT EXISTS alerts (
    patient_id TEXT PRIMARY KEY,
    risk_level TEXT,
    alert_sent INTEGER,
    timestamp TEXT
)
""")

conn.commit()

"""**Insert transcripts**"""

transcripts_dir = "/content/drive/MyDrive/DASYSTEM2/transcript large"

for fname in os.listdir(transcripts_dir):
    if fname.endswith(".txt"):
        patient_id = fname.replace(".txt", "")
        file_path = os.path.join(transcripts_dir, fname)
        with open(file_path, "r", encoding="utf-8") as f:
            transcript = f.read().strip()
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")

        cursor.execute("""
        INSERT OR REPLACE INTO transcripts (patient_id, transcript, timestamp)
        VALUES (?, ?, ?)
        """, (patient_id, transcript, timestamp))

conn.commit()

"""**Insert risk profiling / alert info**"""

profile_dir = "/content/drive/MyDrive/DASYSTEM2/profile_results_biobert_patient_large"

for fname in os.listdir(profile_dir):
    if fname.endswith(".json"):
        patient_id = fname.replace(".json", "")
        file_path = os.path.join(profile_dir, fname)
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        risk_level = data.get("risk_level", "Low")
        alert_sent = 1 if risk_level in ["Medium","High"] else 0
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")

        cursor.execute("""
        INSERT OR REPLACE INTO alerts (patient_id, risk_level, alert_sent, timestamp)
        VALUES (?, ?, ?, ?)
        """, (patient_id, risk_level, alert_sent, timestamp))

conn.commit()
conn.close()

conn = sqlite3.connect('/content/drive/MyDrive/DASYSTEM2/doctor_alert.db')
cursor = conn.cursor()

cursor.execute("SELECT * FROM transcripts LIMIT 5")
print(cursor.fetchall())

cursor.execute("SELECT * FROM alerts WHERE risk_level='High'")
print(cursor.fetchall())

conn.close()

import sqlite3
import pandas as pd


conn = sqlite3.connect('/content/drive/MyDrive/DASYSTEM2/doctor_alert.db')

# Query: join transcripts and alerts
query = """
SELECT t.patient_id, t.transcript, a.risk_level, a.alert_sent, a.timestamp AS alert_time
FROM transcripts t
LEFT JOIN alerts a ON t.patient_id = a.patient_id
ORDER BY t.patient_id
"""

df_report = pd.read_sql(query, conn)
conn.close()

# Show the first 10 rows
print(df_report.head(10))

#save as CSV for submission
df_report.to_csv("/content/drive/MyDrive/DASYSTEM2/patient_report.csv", index=False)